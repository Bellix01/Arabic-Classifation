{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "}\n",
    "\n",
    "url = \"https://www.france24.com/ar/%D8%B1%D9%8A%D8%A7%D8%B6%D8%A9/\"\n",
    "page = requests.get(url, headers=headers)\n",
    "print(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser-lxml = Change html to Python friendly format\n",
    "# Obtain page's information\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "# Find the main div\n",
    "main_div = soup.find('div', attrs={'class': 't-content t-content--page-builder'})\n",
    "\n",
    "# Check if the main div is found\n",
    "if main_div:\n",
    "    sub_divs = main_div.find_all('div', attrs={'class': 'article__title'})\n",
    "\n",
    "    for sub_div in sub_divs:\n",
    "        h3_tag = sub_div.find('h2')\n",
    "        if h3_tag:\n",
    "            data.append(h3_tag.text)\n",
    "\n",
    "data.pop(len(data)-1)\n",
    "# # data\n",
    "mydata = pd.DataFrame(data,columns = ['Text'])\n",
    "mydata.to_csv('output.csv', encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.elsport.com/\"\n",
    "page = requests.get(url, headers=headers)\n",
    "print(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "# print(soup.prettify())\n",
    "\n",
    "# Find the main div\n",
    "main_div = soup.find('ul', class_='listnews news-feed-container newsfeed-list')\n",
    "\n",
    "# Check if the main div is found\n",
    "if main_div:\n",
    "    sub_divs = main_div.find_all('a', attrs={'class':'news-title'})\n",
    "\n",
    "    for sub_div in sub_divs:\n",
    "        h4_tag = sub_div.find('h4')\n",
    "        if h4_tag:\n",
    "            data.append(h4_tag.text)\n",
    "else:\n",
    "    print(\"Main div not found.\")\n",
    "\n",
    "existing_data = pd.read_csv('output.csv')\n",
    "\n",
    "new_data = pd.DataFrame(data, columns=['Text'])\n",
    "\n",
    "\n",
    "# Step 3: Append Data to Existing DataFrame\n",
    "# combined_data = existing_data.append(new_data, ignore_index=True)\n",
    "combined_df = pd.concat([existing_data, new_data], ignore_index=True)\n",
    "\n",
    "# Step 4: Save Combined Data to the Same CSV File\n",
    "combined_df.to_csv('output.csv', encoding='utf-8-sig', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "url = \"https://arabic.rt.com/sport/news/\"\n",
    "page = requests.get(url, headers=headers)\n",
    "print(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the WebDriver (make sure you have the appropriate driver installed)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Load the initial page\n",
    "url = \"https://arabic.rt.com/sport/news/\"\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the \"Load More\" button to be clickable (adjust timeout as needed)\n",
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "# Close the cookie banner if present\n",
    "cookie_banner = driver.find_element(By.CLASS_NAME, \"cookies__banner-wrapper\")\n",
    "if cookie_banner.is_displayed():\n",
    "    cookie_banner.click()\n",
    "\n",
    "# Initialize a variable to store the data\n",
    "data = []\n",
    "\n",
    "# Set the maximum number of clicks\n",
    "max_clicks = 25\n",
    "\n",
    "for click_count in range(max_clicks):\n",
    "    try:\n",
    "        # Wait for the \"Load More\" button to be clickable\n",
    "        load_more_button = wait.until(EC.element_to_be_clickable((By.CLASS_NAME, \"more-button\")))\n",
    "        \n",
    "        # Scroll the \"Load More\" button into view\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView();\", load_more_button)\n",
    "        \n",
    "        # Click the \"Load More\" button\n",
    "        load_more_button.click()\n",
    "        \n",
    "        # Wait for some time to let the content load (you may need to adjust the time based on the page behavior)\n",
    "        time.sleep(5)\n",
    "        \n",
    "        # Get the updated HTML after clicking the button\n",
    "        updated_html = driver.page_source\n",
    "        \n",
    "        # Parse the updated HTML with BeautifulSoup\n",
    "        soup = BeautifulSoup(updated_html, 'html.parser')\n",
    "        \n",
    "        # Extract the data as needed (adjust based on the actual structure of the page)\n",
    "        main_div = soup.find('div', attrs={'class': 'list-news js-listing'})\n",
    "        if main_div:\n",
    "            sub_divs = main_div.find_all('div', attrs={'class': 'list-news_text'})\n",
    "            for sub_div in sub_divs:\n",
    "                h4_tag = sub_div.find('a')\n",
    "                if h4_tag:\n",
    "                    data.append(h4_tag.text)\n",
    "    except Exception as e:\n",
    "        # If the button is not found or clickable, exit the loop\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        break\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Print or use the data as needed\n",
    "# data\n",
    "existing_data = pd.read_csv('output.csv')\n",
    "\n",
    "new_data = pd.DataFrame(data, columns=['Text'])\n",
    "\n",
    "\n",
    "# Step 3: Append Data to Existing DataFrame\n",
    "# combined_data = existing_data.append(new_data, ignore_index=True)\n",
    "combined_df = pd.concat([existing_data, new_data], ignore_index=True)\n",
    "\n",
    "# Step 4: Save Combined Data to the Same CSV File\n",
    "combined_df.to_csv('output.csv', encoding='utf-8-sig', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('output.csv')\n",
    "\n",
    "data['Text'] = data['Text'].str.replace('\\n', '')\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "keywords = [\"تنس\", \"جوائز\", \"ليفربول\", \"أفضل لاعب\", \"رياضة\", \"تنس\", \"كرة السلة\", \"دوري أبطال\", \"رياضي\", \"مباراة\", \"المونديال\", \"كأس أمم\", \"ريال مدريد\", \"يفوز\", \"فوز\"\n",
    "            , \"الملعب\", \"دوري\", \"الدوري\", \"كرة\", \"لاعب\",\"بطولة\",\"بطولة\",\"البطولة\",\"الخسارة\",\"العالم\",\"نهائي\",\"الجزاء\",\"اللعب\",\"إحصاءات\",\"الفوز\",\"مونديال\",\"مدرب\",\"مباريات\"\n",
    "            ,\"المتأهلين\",\"الألعاب\",\"صدارة\",\"المجموعات\",\"الجماهير\",\"منشطات\",\"للفوز\",\"تعادل\",\"نجم\",\"الرياضية\",\"الهدف\",\"القدم\",\"مجريات\",\"القدم\",\"نادي\",\" لقب\",\"المدافع\",\"منتخب\"]\n",
    "\n",
    "\n",
    "\n",
    "def calculate_score(text):\n",
    "    # Calculate the keyword_score based on the presence of keywords in the text\n",
    "    keyword_score = sum(word in text for word in keywords)\n",
    "    \n",
    "    # Additional scoring logic based on text characteristics (e.g., length, sentiment, etc.)\n",
    "    length_score = len(text.split())  # Example: Score based on the number of words\n",
    "    \n",
    "    # Combine the scores with adjusted weights\n",
    "    final_score = 0.8 * keyword_score + 0.1 * length_score\n",
    "    \n",
    "    # Round the final score to one decimal place\n",
    "    rounded_score = round(final_score, 1)\n",
    "    \n",
    "    # Normalize the final score to be between 0 and 10\n",
    "    normalized_score = min(max(rounded_score, 0), 10)\n",
    " \n",
    "    \n",
    "    return normalized_score\n",
    "\n",
    "# Calculate scores for each text in the DataFrame\n",
    "df['Score'] = df['Text'].apply(calculate_score)\n",
    "df[['Text', 'Score']]\n",
    "df.to_csv('output.csv', encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading isri: Package 'isri' not found in index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       دور بطل روبا: نتر ميلان-أتلتيكو درد ونابولي-بر...\n",
       "1       كرة قدم: ئنف بري دور ترك في 19 كان اول بعد عرض...\n",
       "2       دور بطل روبا: يتد ودع بطل خسر على لعب أمام بير...\n",
       "3       شار اهل تحد جدة... سعد حضن نسخ عشر من بطل كأس ...\n",
       "4       لمذ \"فز\" يجر وسم على صلح حكم في سبق لجز فضل لع...\n",
       "                              ...                        \n",
       "3565    اعل تحد ورب لكر قدم \"يويفا\"، انه عزم خصص 331 م...\n",
       "3566    لم بعد نجم ارج ونل يسي شرك في مونديال 2026، قئ...\n",
       "3567    عهد رئس ندي هلل، فهد بن فل، منح كفأ كبر اعب فر...\n",
       "3568    أجل برا بير يونخ وضف اون برل التي كانت قرر يوم...\n",
       "3569    دول غرد يدو ظهر صرف قئد ندي صر، كريستيانو رونا...\n",
       "Name: Text, Length: 3570, dtype: object"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "\n",
    "# Download the NLTK resource if you haven't already\n",
    "import nltk\n",
    "nltk.download('isri')\n",
    "\n",
    "file_path = 'output.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Create an instance of the ISRIStemmer\n",
    "stemmer = ISRIStemmer()\n",
    "\n",
    "# Define a function to apply stemming to a text\n",
    "def stem_text(text):\n",
    "    return ' '.join([stemmer.stem(word) for word in text.split()])\n",
    "\n",
    "df['Text'] = df['Text'].apply(stem_text)\n",
    "\n",
    "df[column_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# first we define a list of arabic and english punctiations that we want to get rid of in our text\n",
    "\n",
    "punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ''' + string.punctuation\n",
    "\n",
    "# Arabic stop words with nltk\n",
    "stop_words = stopwords.words('arabic')\n",
    "\n",
    "arabic_diacritics = re.compile(\"\"\"\n",
    "                             ّ    | # Shadda\n",
    "                             َ    | # Fatha\n",
    "                             ً    | # Tanwin Fath\n",
    "                             ُ    | # Damma\n",
    "                             ٌ    | # Tanwin Damm\n",
    "                             ِ    | # Kasra\n",
    "                             ٍ    | # Tanwin Kasr\n",
    "                             ْ    | # Sukun\n",
    "                             ـ     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [دور, بطل, روبا, نتر, ميلاناتلتيكو, درد, ونابو...\n",
      "1           [كره, قدم, ءنف, بري, دور, اول, عرض, حكم, عدء]\n",
      "2       [دور, بطل, روبا, يتد, ودع, بطل, خسر, علي, لعب,...\n",
      "3       [شار, اهل, تحد, جده, سعد, حضن, نسخ, بطل, كاس, ...\n",
      "4       [لمذ, فز, يجر, وسم, علي, صلح, حكم, سبق, لجز, ف...\n",
      "                              ...                        \n",
      "3565    [اعل, تحد, ورب, لكر, قدم, يويفا, انه, عزم, خصص...\n",
      "3566    [نجم, ارج, ونل, يسي, شرك, مونديال, قءل, انه, ر...\n",
      "3567    [عهد, رءس, ندي, هلل, فهد, بن, فل, منح, كفا, كب...\n",
      "3568    [اجل, برا, بير, يونخ, وضف, اون, برل, كانت, قرر...\n",
      "3569    [دول, غرد, يدو, ظهر, صرف, قءد, ندي, صر, كريستي...\n",
      "Name: Processed_Text, Length: 3570, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "data = pd.read_csv('output.csv')\n",
    "\n",
    "def preprocessing (text):\n",
    "    \n",
    "\n",
    "    #remove punctuations\n",
    "    translator = str.maketrans('', '', punctuations)\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    # remove Tashkeel\n",
    "    text = re.sub(arabic_diacritics, '', text)\n",
    "    text = re.sub('[A-Za-z0-9]',' ',text)\n",
    "    \n",
    "    #normalize\n",
    "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"ؤ\", \"ء\", text)\n",
    "    text = re.sub(\"ئ\", \"ء\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    text = re.sub(\"گ\", \"ك\", text)\n",
    "    #next creating a list of substrings\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "\n",
    "\n",
    "    # tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "data['Processed_Text'] = df['Text'].apply(preprocessing)\n",
    "print(data['Processed_Text'])\n",
    "# data.to_csv('output22.csv', encoding='utf-8-sig', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
